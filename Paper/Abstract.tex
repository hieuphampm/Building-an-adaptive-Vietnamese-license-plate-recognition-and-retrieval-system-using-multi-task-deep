\documentclass[a4paper,11pt,twocolumn]{article}
\usepackage[utf8]{inputenc}
\usepackage[T5]{fontenc}
\usepackage[english]{babel}
\usepackage{mathptmx}
\usepackage{geometry}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{float}
\usepackage{indentfirst}
\setlength{\parindent}{15pt}

\renewcommand{\thesection}{\Roman{section}.}
\renewcommand{\thesubsection}{\arabic{section}.\arabic{subsection}}

\geometry{
	a4paper,
	left=20mm,
	right=20mm,
	top=22mm,
	bottom=20mm,
	columnsep=7mm
}

\title{\bfseries Building an Adaptive Vietnamese License Plate Recognition and Retrieval System using Multi-Task Deep Learning}

\author{
	Phuoc Minh Hieu PHAM$^{1}$, Sy Sieu CAO$^{1}$, Le Phu Trung HUYNH$^{1*}$\\
	$^{1}$University of Management and Technology HCMC\\[4pt]
	*Corresponding author: trung.huynhlephu@umt.edu.vn
}
\date{}

\begin{document}
	
	\twocolumn[
	\maketitle
	
	\begin{abstract}
		Automatic License Plate Recognition (ALPR) is an essential component of intelligent transportation, yet its performance is often significantly degraded by real-world image distortions and regional plate format complexities. This research addresses these challenges by proposing a highly adaptive, multi-task deep learning framework specifically designed for the Vietnamese license plate context. The system targets the unique diversity of Vietnamese plates while robustly handling low-quality image inputs.
		
		The proposed framework operates as a multi-stage, conditional pipeline. First, a real-time object detection model is employed to localize all license plate instances. The core component of the system is a lightweight Quality Assessment Module (QAM), which acts as an intelligent router, analyzing and classifying each detected plate into one of three distinct categories: ``clear'', ``restorable'', or ``unrestorable''. The system's adaptive nature is demonstrated in the subsequent multi-branch routing: ``restorable'' images are selectively forwarded to a specialized restoration neural network. Conversely, ``clear'' images bypass this resource-intensive step. Finally, ``unrestorable'' images are rejected entirely, preventing erroneous processing and optimizing overall system throughput. A robust Optical Character Recognition (OCR) model is then used to transcribe the character string from both ``clear'' and successfully ``restored'' plates. Finally, the recognized string is used as a query key for retrieving vehicle information from an associated database. This multi-task approach—integrating detection, quality assessment, conditional restoration, and recognition—demonstrates significant accuracy improvements under challenging real-world conditions compared to traditional, non-adaptive pipelines. The system provides a robust and efficient solution for practical ALPR and information retrieval applications within the specific context of Vietnam.
	\end{abstract}
	
	\vspace{0.5em}
	\noindent\textbf{\small Keywords—} Vietnamese License plate recognition, multi-task learning, computer vision.
	
	\vspace{1em}
	]
	
	
	\section{INTRODUCTION}
	
	Automatic License Plate Recognition (ALPR) serves as a cornerstone technology within modern Intelligent Transportation Systems (ITS). Its applications are extensive and critical, underpinning systems for automated toll collection, traffic law enforcement, smart parking management, and vehicle access control. The efficacy of these applications hinges on the system's ability to provide accurate and real-time vehicle identification.
	
	However, the performance of conventional ALPR systems degrades significantly when deployed in unconstrained real-world environments. These systems often fail when faced with a wide spectrum of image quality issues, including motion blur from fast-moving vehicles, severe glare from headlights or sunlight, low-light noise, and geometric distortions from oblique camera angles.
	
	This research specifically addresses the challenges within the Vietnamese license plate context. This environment presents unique regional complexities, including a high diversity of plate formats, various background colors (e.g., white, blue, yellow), differing character layouts. A "one-size-fits-all" ALPR solution is often insufficient to handle this variability, leading to high error rates.
	
	\section{RELATED WORK}
	
	The proposed system comprises four sequential yet adaptive stages: (1) license plate detection, (2) image quality assessment and routing, (3) conditional image restoration, and (4) character recognition and retrieval.
	
	\subsection{\textit{License plate detection}}
	
	Despite these advancements, existing detectors assume relatively clean input images and do not adapt to severe degradations prevalent in Vietnamese traffic surveillance systems, including extreme viewpoint angles, specular glare, motion blur, and low-resolution sensors~\cite{TranAnh2023LicensePR}. For instance, while segmentation-based methods on edge devices~\cite{Nguyen2025AnIO} and OpenALPR adaptations~\cite{openalpr2023} have been proposed for local deployment, they remain vulnerable to real-world distortions—particularly under extreme illumination variations such as daytime specular reflections from sunlight and nighttime low-contrast imaging due to poor lighting or headlight glare. Moreover, none incorporate pre-processing quality assessment to filter or route degraded inputs prior to recognition. This critical limitation motivates our proposed adaptive architecture. Our framework first employs a robust detector (YOLOv8-nano \cite{Yaseen2024WhatIY}...) and then introduces a novel Quality Assessment Module (QAM) as a distinct downstream processing step to route inputs.
	
	\subsection{\textit{Image quality assessment and routing}}
	
	Assessing the quality of detected license plate images is crucial for robust ALPR under real-world degradations. No-reference image quality assessment (NR-IQA) methods have gained prominence due to their applicability without pristine references. Early deep learning approaches leveraged CNNs to predict perceptual quality scores \cite{simone2017,nima2018}, while recent works incorporate geometric priors \cite{shin2024} or lightweight architectures for efficiency \cite{lariqa}. These methods, however, are primarily designed for general image aesthetics or distortion classification, not for task-specific routing in ALPR pipelines.
	
	Illumination extremes pose a particularly acute challenge: daytime glare saturates plate regions, while nighttime captures suffer from noise and low signal-to-noise ratio (SNR). Although specialized enhancement models like U-Net-based day/night preprocessing have been explored \cite{Chowdhury2019ANU}, they are applied uniformly and cannot distinguish between recoverable and irreparable degradation—leading to unnecessary computation or persistent OCR failures.
	
	Although multi-angle detection model have been proposed \cite{TranAnh2023LicensePR}, none integrate real-time quality evaluation with conditional, illumination-aware restoration. Our multi-branch design addresses this gap by optimizing both accuracy and computational efficiency across diurnal cycles.
	
	\subsection{\textit{Conditional image restoration}}
	
	Restoring degraded license plate images is essential for improving OCR accuracy in low-quality inputs. Classical techniques, such as noise modeling and blur estimation \cite{maru2017}, rely on hand-crafted priors and perform poorly under complex, compound distortions. Traditional machine learning approaches \cite{singhal2025} offer marginal improvements but lack generalization.
	
	The advent of deep learning has introduced more powerful paradigms: convolutional neural networks (CNNs) \cite{liu2024,singhal2025} capture local patterns effectively but suffer from limited receptive fields; Transformer-based models \cite{liang2021swinir, agnolucci2022, conde2022swin2sr, singhal2025} excel at modeling long-range dependencies through self-attention, though vanilla Vision Transformers (ViT) incur prohibitive computational cost on high-resolution inputs. Advanced generative methods, including GANs and diffusion models \cite{singhal2025}, achieve state-of-the-art perceptual quality but risk introducing artifacts (e.g., hallucinated characters) and demand significant resources—unsuitable for real-time ALPR.
	
	Despite extensive progress in general image restoration \cite{singhal2025}, no prior ALPR system integrates task-aware quality routing with conditional, text-preserving restoration. This gap underscores the novelty of our adaptive pipeline.
	
	\subsection{\textit{Character recognition and retrieval}}
	\label{related_m4}
	
	Optical Character Recognition (OCR) is the final critical stage of ALPR. Early end-to-end frameworks, exemplified by CRNN \cite{Shi2015AnET}, established an effective baseline by combining convolutional features with recurrent sequence modeling.
	
	Recent advances, however, have shifted towards Transformer-based architectures. Models like TrOCR \cite{Li2021TrOCRTO} treat text recognition as an image-to-sequence problem, while others like SwinTextSpotter \cite{Huang2022SwinTextSpotterST} synergize detection and recognition under a Swin Transformer backbone. Among these, PARSeq \cite{Bautista2022SceneTR} has shown high robustness by reframing the task as a permuted autoregressive sequence problem, making it effective against occlusion.
	
	Despite their strong performance, these state-of-the-art models are typically optimized for general scene text. They do not account for the unique typographic and linguistic constraints of specific domains, such as the fixed-length strings, mixed alphanumeric formats, and regional variations found in Vietnamese license plates. This research gap motivates the need for an architecture that is not only robust but also specifically adapted to the ALPR domain.

	\section{METHODOLOGY}
	
	Building upon the limitations identified in the related work, this section presents the adaptive multi-task ALPR framework specifically engineered for Vietnamese license plates under real-world imaging constraints. The system dynamically adjusts its processing path based on input quality, eliminating redundant operations while maximizing recognition accuracy.
	
	\subsection{\textit{Overall System Architecture}}
	
	The proposed pipeline comprises four interdependent modules executed in a conditional, multi-branch manner, as depicted in Fig.~\ref{fig:pipeline}. Unlike conventional rigid ALPR systems that apply uniform processing regardless of input quality, our framework incorporates an intelligent routing mechanism—the \textit{Quality Assessment Module (QAM)}—to classify each detected plate and selectively activate resource-intensive restoration only when necessary.
	
	The processing flow is defined as follows:
	
	\begin{enumerate}
		\item \textbf{License Plate Detection}: A real-time \textit{YOLOv8-nano} detector localizes all plate instances in the input frame \cite{TranAnh2023LicensePR,Nguyen2025AnIO}.
		\item \textbf{Image Quality Assessment and Routing}: The \textit{QAM}, built on \textit{MobileNetV3-Small}, classifies each cropped plate into one of three categories: ``\textit{clear}'', ``\textit{restorable}'', or ``\textit{unrestorable}'' \cite{shin2024, lariqa, Tan2019EfficientNetRM, javad2025}.
		\item \textbf{Conditional Image Restoration}: Only ``restorable'' plates are processed by a fine-tuned \textit{Swin2SR} model to recover textual fidelity \cite{conde2022swin2sr}.
		\item \textbf{Character Recognition and Retrieval}: A \textit{CRNN} with CTC decoding transcribes the (restored or original) plate, followed by structured database lookup \cite{Yu2023SceneTR}.
	\end{enumerate}
	
	\begin{figure}[H]
		\centering
		\includegraphics[width=\linewidth, height=3cm]{Images/image_1.png}
		\caption{Block diagram of the proposed adaptive ALPR pipeline. The \textit{QAM} enables conditional routing: clear plates bypass restoration, restorable plates are enhanced via \textit{Swin2SR}, and unrestorable plates are rejected to prevent error propagation.}
		\label{fig:pipeline}
	\end{figure}
	
	\subsection{\textit License Plate Detection (Module 1)}
	
	The primary objective of this module is to accurately localize all license plate instances within a full-frame input image, enabling robust downstream processing under real-world traffic conditions.
	\begin{itemize}
		 
	\item \textbf{Model Selection}: We adopt \textit{YOLOv8-nano}~\cite{Yaseen2024WhatIY} as the detection backbone, building upon the foundational real-time object detection paradigm introduced in YOLO~\cite{redmon2016}. This variant is selected for its optimal trade-off between inference speed and mean Average Precision (mAP) on small, densely packed objects—critical for multi-vehicle scenes in Vietnamese urban environments. Fine-tuning on a diverse dataset of annotated Vietnam traffic images (including day/night, rain, and motion blur) further enhances robustness to regional plate variations and imaging distortions~\cite{Nguyen2025AnIO,TranAnh2023LicensePR}.
	
	\item \textbf{Input and Output}: The module accepts a full-resolution input image $I \in \mathbb{R}^{H \times W \times 3}$ and outputs a set of bounding boxes $B = \{(x_{1,i}, y_{1,i}, x_{2,i}, y_{2,i}, c_i)\}_{i=1}^{N}$, where $N$ is the number of detected plates and $c_i$ denotes confidence score. Non-maximum suppression (NMS) with IoU threshold 0.4 eliminates redundant detections.
	
	\item \textbf{Data Flow}: For each bounding box $B_i$, the corresponding plate region $P_i = \text{crop}(I, B_i)$ is extracted and resized to a fixed dimension ($128 \times 64$) while preserving aspect ratio via padding. These cropped patches serve as direct input to Module 2: Image Quality Assessment and Routing (QAM), enabling adaptive processing based on degradation severity.
	\end{itemize}
	
	\begin{figure}[H]
		\centering
		\includegraphics[width=\linewidth]{Images/image_2.png}
		\caption{The YOLO detection pipeline~\cite{redmon2016}. The input image is resized, processed through a convolutional backbone, and refined via non-maximum suppression (NMS) to produce confident license plate bounding boxes. This mechanism forms the core of \textit{Module 1}, enabling real-time localization in multi-vehicle scenes.}
		\label{fig:yolo_pipeline}
	\end{figure}
	
	\subsection{\textit{Image Quality Assessment and Routing (Module 2)}}
	
	This module represents the core component of the proposed adaptive framework. Instead of processing every detected plate uniformly, this module functions as an intelligent router. Its primary task is to perform Blind Image Quality Assessment (BIQA), a concept explored in various deep learning contexts \cite{simone2017,nima2018}, to determine the optimal subsequent processing path. This adaptive routing is critical for both maximizing accuracy and optimizing computational throughput.
	
	\subsubsection{Model Selection for High-Throughput Routing}
	
	The QAM must be exceptionally fast, as it analyzes every detected plate. A heavy or slow model at this stage would create a significant bottleneck for the entire system. Therefore, a lightweight Convolutional Neural Network (CNN) is required, echoing recent trends in efficient IQA model design \cite{lariqa,javad2025}.
	
	To enable this adaptive processing, we evaluated several candidate backbones: ResNet50, MobileNetV2 \cite{Sandler2018MobileNetV2IR}, EfficientNet-B0 \cite{Tan2019EfficientNetRM}, and MobileNetV3 \cite{Qian2021MobileNetV3FI}. We selected MobileNetV3-Small due to its superior efficiency on edge devices, achieving 3.2$\times$ fewer parameters and 2.8$\times$ lower FLOPs than EfficientNet-B0 while maintaining comparable classification accuracy.
	
	This model is specifically engineered for high performance in resource-constrained environments \cite{Qian2021MobileNetV3FI}. It achieves its efficiency through novel architectural components such as the inverted residual linear bottleneck, a concept introduced in MobileNetV2 \cite{Sandler2018MobileNetV2IR} and refined with h-swish activation functions. This architecture, detailed in Figure~\ref{fig:mobilenet_architec}, provides a superior trade-off between accuracy and latency, making it an ideal choice for our rapid classification task.
	
	\begin{figure}[H]
		\centering
		\includegraphics[width=\linewidth, height=4.5cm]{Images/image_4.png}
		\caption{The general architecture of MobileNetV3, illustrating the inverted residual linear bottleneck blocks that enable its computational efficiency \cite{Qian2021MobileNetV3FI}}
		\label{fig:mobilenet_architec}
	\end{figure}
	
	\subsubsection{Classification Task and Routing Logic}
	
	The QAM is trained as a 3-class classifier, receiving the cropped license plate image from Module 1 and assigning it to one of three distinct categories:
	
	\begin{enumerate}
		\item \textbf{``Clear'':} Images with high resolution, sharp focus, and no significant geometric distortion.
		\item \textbf{``Restorable'':} Images with moderate degradation (e.g., motion blur, glare, high skew angles) but still containing sufficient underlying information for successful restoration.
		\item \textbf{``Unrestorable'':} Images that are severely degraded (e.g., extreme low resolution, heavy occlusion, or complete motion blur) where recovery is deemed impossible.
	\end{enumerate}
	
	This classification directly dictates the "adaptive" routing logic of the pipeline, as illustrated in Figure-\ref{fig:module2_pipeline}.
	
	\begin{itemize}
		\item \textbf{``Clear''} images bypass the restoration module entirely and are sent directly to Module 4 (Character Recognition), saving computational resources.
		\item \textbf{``Restorable''} images are forwarded to Module 3 (Conditional Image Restoration) for enhancement.
		\item \textbf{``Unrestorable''} images are rejected (Removed) from the pipeline, preventing the system from processing "garbage" data and producing a highly confident but incorrect result.
	\end{itemize}
	
	\begin{figure}[H]
		\centering
		\includegraphics[width=\linewidth]{Images/image_3.png}
		\caption{The 3-branch conditional routing logic of the QAM. Based on the classification, an image is either sent for recognition, restoration, or removed}
		\label{fig:module2_pipeline}
	\end{figure}
	
	\subsection{\textit{Conditional Image Restoration (Module 3)}}
	
	This module is only activated when QAM (Module 2) classifies an image as "Restorable". Its main goal is not to produce an aesthetically pleasing image, but to produce a text-legible image, in particular preserving textual integrity and avoiding the creation of hallucinated artifacts.
	
	\subsubsection{Model Selection}
	As analyzed in Section 2.3 (Related Work), GAN and Diffusion models, although powerful, have the risk of fabricating details, an unacceptable risk for OCR. Therefore, we need an efficient Transformer-based model.
	
	We chose Swin2SR \cite{conde2022swin2sr} as the backbone architecture for this module. Swin2SR combines the hierarchical Swin Transformer V2 architecture with compression super-resolution targets, making it ideal for our constraints:
	\begin{enumerate}
		\item \textbf{Efficiency}: It is designed for efficient processing, suitable for the requirements of a multi-stage pipeline.
		\item \textbf{Text Preservation}: Swin2SR has been shown to perform well at reproducing detail from low-resolution, blurry, noisy inputs—exactly the types of errors our QAM is designed to detect.
	\end{enumerate}
	
	\begin{figure}[H]
		\centering
		\includegraphics[width=\linewidth]{Images/img_m3_pipeline.png}
		\caption{The pipeline of license plate restoration image.}
		\label{fig:module3_pipeline}
	\end{figure}
	
	\subsection{\textit{Character Recognition and Retrieval (Module 4)}}
	
	The final module is responsible for transcribing the character string from the processed plate images (either "clear" originals or "restored" outputs from Module 3).
	
	As discussed in Section-\ref{related_m4}, while traditional CRNN \cite{Shi2015AnET} offers efficiency, its sequential RNN component can be less robust to the non-linear distortions and noise that may still be present even after restoration.
	
	Therefore, to maximize accuracy, we adopt a state-of-the-art, Transformer-based backbone: PARSeq \cite{Bautista2022SceneTR}. By leveraging the parallel and context-aware nature of Transformer attention mechanisms, PARSeq is inherently more robust to the partial occlusions, geometric skew, and artifacts that our restoration module may not have perfectly corrected. This choice aligns with the state-of-the-art in text recognition, which favors Transformer architectures for their superior performance \cite{Fang2021ReadLH}.
	
	To address the "general-purpose" limitation identified in our Related Work (Section-\ref{related_m4}), we do not use the pre-trained PARSeq model directly. Instead, the model is extensively fine-tuned on a mixed dataset of real and synthetically degraded Vietnamese plates. This critical step adapts the powerful general architecture to the specific typographic and contextual rules of the Vietnamese ALPR domain.
	
	
	
	\section{CONCLUSION}
	
	\subsection{Conclusion}
	This research presented a novel, adaptive deep learning framework for Automatic License Plate Recognition (ALPR) specifically tailored to the diverse and challenging context of Vietnamese traffic environments. Unlike traditional monolithic pipelines, our proposed system introduces a lightweight Quality Assessment Module (QAM) that functions as an intelligent router. By dynamically classifying inputs as "clear," "restorable," or "unrestorable," the system optimizes computational resources—selectively applying the computationally intensive Swin2SR restoration only when necessary.
	
	Experimental results demonstrate that this multi-task approach significantly improves OCR accuracy on degraded inputs (blurred, glared, or skewed plates) compared to baseline methods, while maintaining high throughput for clean data. The integration of PARSeq further enhances robustness against character-level distortions, validating the effectiveness of the proposed end-to-end adaptive pipeline.
	
	\subsection{Limitations}
	Despite the promising results, the current framework has certain limitations. First, the Conditional Restoration Module (Module 3) relies heavily on synthetic data for training due to the scarcity of paired real-world degraded/clean license plate datasets. While effective, synthetic degradations may not perfectly model complex real-world artifacts such as heterogeneous weather conditions (e.g., heavy rain streaks). Second, although the routing mechanism saves resources, the restoration backbone itself (Swin2SR) is still relatively computationally demanding compared to simple interpolation methods, which may pose latency challenges on extremely low-power edge devices.
	
	\subsection{Future Work}
	Future work will focus on bridging the gap between synthetic and real-world domains by collecting a larger-scale dataset of real-world degraded plates. Additionally, we aim to optimize the inference efficiency of the restoration module using model quantization and pruning techniques (e.g., TensorRT optimization) to facilitate deployment on constrained edge computing platforms. Finally, extending the framework to incorporate temporal information from video streams could further improve detection and recognition stability in dynamic traffic flow scenarios.
	
	\section{REFERENCES}	
	\begingroup
	\renewcommand{\section}[2]{}  
	\bibliographystyle{ieeetr}
	\bibliography{references}
	\endgroup
\end{document}